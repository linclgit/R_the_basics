---
title: "The dplyr package"
subtitle: "Swirl course notes"
---

``dplyr`` is a fast and powerful R package written by Hadley Wickham and Romain Francois that provides **a consistent and concise grammar for manipulating tabular data**.  
  
One unique aspect of ``dplyr`` is that the same set of tools allow you to work with tabular data from a variety of sources, including data frames, data tables, databases and multidimensional arrays.  
  
CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R" (http://cran.rstudio.com/).  
RStudio maintains one of these so-called 'CRAN mirrors' and they generously make their download logs publicly available (http://cran-logs.rstudio.com/).   

#### The data used here is the log from July 8, 2014, which contains information on roughly 225,000 package downloads.  
```{r}
mydf <- read.csv (file = "../data/2014-07-08.csv", stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
```
```{r message=FALSE, warning=FALSE}
library (dplyr)
packageVersion ("dplyr")
```
  
load the data into what the package authors call a '**data frame tbl**'  
```{r}
cran <- as_tibble (mydf) #tbl_df() has been deprecated
rm ("mydf")
cran
```
This output is much more informative and compact than what we would get if we printed the original data frame to the console.  
First, we are shown the **class** and **dimensions** of the dataset. Just below that, we get a **preview** of the data.  
Instead of attempting to print the entire dataset, ``dplyr`` just shows us the first 10 rows of data and only as many columns as fit neatly in our console.  
At the bottom, we see the **names** and **classes** for any variables that didn't fit on our screen.  

#### "The main advantage to using a tbl_df over a regular data frame is the printing."  
#### Specifically, dplyr supplies five 'verbs' that cover most fundamental data manipulation tasks: ``select()``, ``filter()``, ``arrange()``, ``mutate()``, and ``summarize()``  
  
According to the "**Introduction to dplyr**" vignette written by the package authors,"**The dplyr philosophy is to have small functions that each do one thing well.**"  
  
As may often be the case, **particularly with larger datasets**, we are only interested in some of the variables.
  
### Select columns: ``select()``  
```{r}
select (cran, ip_id, package, country)
```
Things to notice:  
1. the function knows we are typing column names from the dataset  
2. the columns are returned to us in the order we specified  

```{r}
select (cran, r_arch:country)
```
  
Normally, the ``:`` notation is reserved for numbers, but ``select()`` allows you to **specify a sequence of columns** this way, which can save a bunch of typing.  
```{r}
select (cran, r_arch:country)
```

#### select the same columns in **reverse** order
```{r}
select (cran, country:r_arch)
```
  
#### specify the columns we want to throw away.
```{r}
select (cran, -time)
```
  
omit all columns from date through size
```{r}
select (cran, -(date:size))
```
  
### Select rows: ``filter()``
```{r}
filter (cran, package == "swirl")
```
Things to notice:  
1. The ``==`` operator asks whether the thing on the left is equal to the thing on the right. If yes, then it returns TRUE. If no, then FALSE. In this case, package is an entire vector (column) of values, so ``package == "swirl"`` returns a vector of TRUEs and FALSEs.   
2. Unlike the usual usage, ``==`` now can apply to strings  
  
#### specify as many conditions as you want, separated by commas  
```{r}
filter (cran, r_version == "3.1.1", country == "US")
filter (cran, r_version <= "3.0.2", country == "IN")
```

#### request multiple conditions  
```{r}
filter (cran, country == "US" | country == "IN")
filter (cran, size > 100500, r_os == "linux-gnu")
```

#### select by excluding things we don't want  
```{r}
filter (cran, !is.na (r_version))
```

### Order the rows: ``arrange()``
```{r}
cran2 <- select (cran, size:ip_id)
```

#### order the rows in **ascending** order  
```{r}
arrange (cran2, ip_id)
```

#### order the rows in **descending** order  
```{r}
arrange (cran2, desc (ip_id))
```

#### arrange the data according to the values of multiple variables  
```{r}
arrange (cran2, package, ip_id)
arrange (cran2, country, desc (r_version), ip_id)
```

### Create new variables: ``mutate()``
```{r}
cran3 <- select (cran, ip_id, package, size)
cran3
```

#### add a new variable based on the value of one variable in a dataset  
```{r}
mutate (cran3, size_mb = size / 2^20)
mutate (cran3, correct_size = size + 1000)
```

#### add 2 new variable in which the value computed for the second new column is based on the first new column  
```{r}
mutate (cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
```

### Collapses the dataset to a single row: ``summarize()``  
```{r}
summarize (cran, avg_bytes = mean (size))
```
``summarize()`` is most useful when working with **data that has been grouped by the values of a particular variable**.  

### ``summarize()`` + ``group_by()``  
The main idea behind grouping data is that you want to break up your dataset into groups of rows based on the values of one or more variables.  
```{r}
by_package <- group_by (cran, package)
by_package
```
At the top of the output above, you'll see '**Groups: package**', which tells us that this tbl has been grouped by the package variable. **Everything else looks the same**, but now any operation we apply to the grouped data will take place on a per package basis  

#### summarize again
```{r}
summarize (by_package, mean (size))
```
Instead of returning a single value, ``summarize()`` now returns the mean size for EACH package in our dataset.  
  
#### summarize more columns
```{r}
pack_sum <- summarize (by_package,
                       count = n (), # total number of rows for each package
                       unique = n_distinct (ip_id), # total number of unique downloads for each package
                       countries = n_distinct (country), # number of countries in which each package was downloaded
                       avg_bytes = mean (size))
pack_sum
```

#### Solve a real problem: which packages were most popular on the day these data were collected  
Let's start by isolating the top 1% of packages, based on the total number of downloads as measured by the 'count' column  
We need to know the value of 'count' that splits the data into the top 1% and bottom 99% of packages based on total downloads. In statistics, this is called **the 0.99, or 99%, sample quantile**.  
```{r}
quantile (pack_sum$count, probs = 0.99)
```

Now we can isolate only those packages which had more than 679 total downloads.  
```{r}
top_counts <- filter (pack_sum, count > 679)
top_counts
```

``arrange()`` the rows of top_counts based on the 'count' column  
```{r}
top_counts_sorted <- arrange (top_counts, desc(count))
top_counts_sorted
```
If we use total number of downloads as our metric for popularity, then the above output shows us the most popular packages downloaded from the RStudio CRAN mirror on July 8, 2014. Not surprisingly, ggplot2 leads the pack with 4602 downloads, followed by Rcpp, plyr, rJava  
  
...And if you keep on going, you'll see swirl at number 43, with 820 total downloads.  
  
Perhaps we're more interested in the number of *unique* downloads on this particular day. In other words, if a package is downloaded ten times in one day from the same computer, we may wish to count that as only one download.  
```{r}
quantile (pack_sum$unique, probs = 0.99)
```

```{r}
top_unique <- filter (pack_sum, unique > 465)
top_unique
```
to see which packages were downloaded from the greatest number of unique IP addresses  
```{r}
top_unique_sorted <- arrange (top_unique, desc (unique))
top_unique_sorted
```
Now Rcpp is in the lead, followed by stringr, digest, plyr, and ggplot2. swirl moved up a few spaces to number 40, with 698 unique downloads.  

**Chaining** allows you to string together multiple function calls in a way that is compact and readable, while still accomplishing the desired result.  

It's worth noting that we sorted primarily by country, but used avg_bytes (in ascending order) as a tie breaker.
This means that if two packages were downloaded from the same number of countries, the package with a smaller average download size received a higher ranking.
```{r}
top_countries <- filter (pack_sum, countries > 60)
result1 <- arrange (top_countries, desc (countries), avg_bytes)
```

```{r}
result3 <-
  cran %>%
  group_by(package) %>%
  summarize(count = n(),
            unique = n_distinct(ip_id),
            countries = n_distinct(country),
            avg_bytes = mean(size)) %>%
  filter(countries > 60) %>%
  arrange(desc(countries), avg_bytes)
```
In this script, we've used a special chaining operator, ``%>%``, which was originally introduced in the magrittr R package and has now become a key component of dplyr. The benefit of ``%>%`` is that it allows us to chain the function calls in a linear fashion. The code to the right of ``%>%`` operates on the result from the code to the left of ``%>%``  
It looks like Rcpp is on top with downloads from 84 different countries, followed by digest, stringr, plyr, and ggplot2. swirl jumped up the rankings again, this time to 27th.  

### chaining practices  
#### 1. select columns  
```{r}
cran %>%
  select(ip_id, country, package, size) %>%
  print
```

#### 2. add a column called size_mb that contains the size of each download in megabytes (i.e. size / 2^20)
```{r}
cran %>%
  select(ip_id, country, package, size) %>%
  mutate(size_mb = size / 2^20) %>%
  print
```

#### 3. select all rows for which size_mb is less than or equal to (<=) 0.5.
```{r}
cran %>%
  select(ip_id, country, package, size) %>%
  mutate(size_mb = size / 2^20) %>%
  filter(size_mb <= 0.5) %>%
  print
```

#### 4. arrange() the result by size_mb, in descending order.
```{r}
cran %>%
  select(ip_id, country, package, size) %>%
  mutate(size_mb = size / 2^20) %>%
  filter(size_mb <= 0.5) %>%
  arrange(desc (size_mb)) %>%
  print
```

In this lesson, you learned about grouping and chaining using dplyr. You combined some of the things you learned in the previous lesson with these more advanced ideas to produce concise, readable, and highly effective code. Welcome to the wonderful world of dplyr!  







